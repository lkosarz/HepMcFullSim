# submitFullSim_HepMC.job -- submit sim

Universe        		= vanilla
GetEnv          		= False
#InteractiveJob 			= False

# Use singularity image
+SingularityImage="/cvmfs/singularity.opensciencegrid.org/eicweb/eic_xl:nightly"
#+SingularityImage="/cvmfs/singularity.opensciencegrid.org/eicweb/jug_xl:23.11-stable"

# The requirement line specifies which machines we want to run this job on.  Any arbitrary classad expression can be used.
Requirements    = (CPU_Speed >= 1)

# Rank is an expression that states how to rank machines which have already met the requirements expression. Essentially, 
# rank expresses preference.  A higher numeric value equals better rank.  Condor will give the job the machine with the highest rank.
Rank		= CPU_Speed

# Jobs by default get 1.4Gb of RAM allocated, ask for more if needed but if a job needs
#more than 2Gb it will not be able to run on the older nodes
#request_memory = 1800M

# If you need multiple cores you can ask for them, but the scheduling may take longer the "larger" a job you ask for
#request_cpus = 1

# Used to give jobs a directory with respect to file input and output.
Initialdir      = /gpfs/mnt/gpfs02/eic/lkosarzew/Calorimetry/Simulations/HepMcFullSim

executable              = runFullSimBatch_HepMC_list.sh
arguments               = files_HepMC.list output 1 $(Process) $(Cluster)

transfer_input_files    = runFullSimBatch_HepMC_list.sh,S3setup.sh,data/files_HepMC.list
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT_OR_EVICT
transfer_output_files   = outputFull


Error                   = outputFull/stderr/$(Cluster)_$(Process).err
Output                  = outputFull/stdout/$(Cluster)_$(Process).out
Log                     = outputFull/log/$(Cluster)_$(Process).log

#+Experiment     = "experiment"
#+Job_Type       = "cas"

Queue 50